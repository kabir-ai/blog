{
  
    
        "post0": {
            "title": "Part1 Understanding RxRx19 (COVID-19) dataset by a non-life scientist machine learning practitioner",
            "content": "Summary . The goal of this blog post is to understand the SARS-CoV-2 dataset (RxRx19) released by RecursionPharma from a machine learning / data science prespective. . 1. Data generation . The First Morphological Imaging Dataset on SARS-CoV-2 Virus. . COVID-19 is the biggest challenge faced by our society in recent times. In order to combat COVID-19 as fast as possible life scientists wanted to evaluate which drugs, that are already approved by the FDA can prove to be therapeutic in the case of a COVID-19 infection. They looked at a total of 1,670 compounds. . The process of preparing human cells to be tested for COVID-19 is typically performed by life-scientists. I will try to describe this process via this blog post in layman terms in order to better understand how the images were generated. It is important for a machine learning scientist to understand the process with which the data is generated. This enables the machine learning scientist to experiment with relevant computer vision deep learning architectures. . Chronological steps . 1. The first step . Epithelial cells were extracted from human lungs and kidneys.“Epithelial cells are cells that come from surfaces of your body, such as your skin, blood vessels, urinary tract, or organs. They serve as a barrier between the inside and outside of your body, and protect it from viruses” [1] Then a process called seeding was started. During this process a life scientist takes equal concentrations of cells and add them to a container. The goal is to have an equal concentration of cells in all containers with only 1 layer of cells, referred to as monolayers. . 2. The Second step . After 2 hours post seeding, chemical suppressor screens were done. Each FDA approved compound (the chemical) was added to the seeded cells from step 1 in six half-log doses with six replicates of each dose. Half log dose is a measure of stepwise addition of a compound also known as serial dilution. Six half log doses imply that for each FDA approved compound 6 different concentrations of the compound were administered in the seeded cell solution. For each variation of the dose there were exactly 6 more replicas. to be explained better . 3. The third step . After 24 hours, the chemical suppressor screens from step two were inoculated with the COVID-19 virus. These were further left alone for 96 hours (4 days) before a process called fixing, staining and imaging was done. Fixing is the process of attaching the cells to the petri dish before imaging. Staining is the process of adding a colored compound to the petri dish. Then the cells were imaged which can be analyzed either manually or via computer vision machine learning models. . 2. The Dataset . There is a total of 305,520 images (~ 450 GB in size). Each image is 1024 x 1024 (height x width) x 3 (&#39;RGB&#39;) . for col in metadata.columns: print(f&quot; No. of unique values in {col} = {metadata[col].nunique()}&quot;) . No. of unique values in site_id = 305520 No. of unique values in well_id = 76380 No. of unique values in cell_type = 2 No. of unique values in experiment = 4 No. of unique values in plate = 27 No. of unique values in well = 1340 No. of unique values in site = 4 No. of unique values in disease_condition = 3 No. of unique values in treatment = 1670 No. of unique values in treatment_conc = 14 . Example of the image paths, such as HRCE-1/Plate1/AA02_s1_w3.png, can be read as: [2] . Experiment Name: Cell type and experiment number (HRCE experiment 1) Plate Number (1) Well location on plate (column AA, row 2) Site (2) Channel (3) . All five channels (w1 - w5) make up an single image of a given site. . There are 5 channels for each image, where each channel represents a different type of stain used . . Channel 1: Hoechst . Channel 2: ConA . Channel 3: Phalloidin . Channel 4: Syto14 . Channel 5: WGA . # collapse_hide img_ch1 = &#39;/content/drive/My Drive/RxRx19a/Images/HRCE-1/Plate1/AA02_s1_w1.png&#39; img_ch2 = &#39;/content/drive/My Drive/RxRx19a/Images/HRCE-1/Plate1/AA02_s1_w2.png&#39; img_ch3 = &#39;/content/drive/My Drive/RxRx19a/Images/HRCE-1/Plate1/AA02_s1_w3.png&#39; img_ch4 = &#39;/content/drive/My Drive/RxRx19a/Images/HRCE-1/Plate1/AA02_s1_w4.png&#39; img_ch5 = &#39;/content/drive/My Drive/RxRx19a/Images/HRCE-1/Plate1/AA02_s1_w5.png&#39; img_cv2_ch1 = cv2.imread(img_ch1,1) img_cv2_ch2 = cv2.imread(img_ch2,1) img_cv2_ch3 = cv2.imread(img_ch3,1) img_cv2_ch4 = cv2.imread(img_ch4,1) img_cv2_ch5 = cv2.imread(img_ch5,1) fig, (ax1, ax2,ax3,ax4,ax5) = plt.subplots(1, 5,figsize=(30,20)) ax1.imshow(img_cv2_ch1) ax1.set_title(&#39;Hoechst&#39;,fontweight=&quot;bold&quot;, size=20) ax2.imshow(img_cv2_ch2) ax2.set_title(&#39;ConA&#39;,fontweight=&quot;bold&quot;, size=20) ax3.imshow(img_cv2_ch3) ax3.set_title(&#39;Phalloidin&#39;,fontweight=&quot;bold&quot;, size=20) ax4.imshow(img_cv2_ch4) ax4.set_title(&#39;Syto14&#39;,fontweight=&quot;bold&quot;, size=20) ax5.imshow(img_cv2_ch5) ax5.set_title(&#39;WGA&#39;,fontweight=&quot;bold&quot;, size=20) plt.show() . . plot the composite image . # collapse_hide plt.figure(figsize=(20,20)) dst = cv2.add(img_cv2_ch1,img_cv2_ch2) dst = cv2.add(dst,img_cv2_ch3) dst = cv2.add(dst,img_cv2_ch4) dst = cv2.add(dst,img_cv2_ch5) from google.colab.patches import cv2_imshow # cv2_imshow(dst) plt.imshow(dst) plt.show() . . the dataset contains three viral conditions . active virus | irradiated | mock | # collapse_hide metadata.disease_condition.value_counts() . . Active SARS-CoV-2 280376 UV Inactivated SARS-CoV-2 9120 Mock 9120 Name: disease_condition, dtype: int64 . metadata.cell_type.value_counts() . HRCE 284080 VERO 21440 Name: cell_type, dtype: int64 . Isolate the kidney cells . HRCE = Human kidney cells . VERO = African green money kidney cells . hrce = metadata[metadata.cell_type == &#39;HRCE&#39;] . checking the distribution of labels ( Active vs Mock vs Inactive Virus ) . #collapse_hide hrce.disease_condition.value_counts() . . Active SARS-CoV-2 260696 UV Inactivated SARS-CoV-2 8480 Mock 8480 Name: disease_condition, dtype: int64 . 3. What can be learned from this dataset using machine learning ? . For a non-life scientist machine learning practitioner, the first question that comes to mind is, what specific features of an image helps the model understand whether the virus is active vs inactive? . In the next blog, i will show how one can train a model to identify active from inactive viruses. Further, i will use contemporary machine learning explainability techniques to understand the model predictions. . Reference . [1] https://www.healthline.com/health/epithelial-cells-in-urine . [2] https://gist.github.com/bmabey/ae215f5c154cbc5c3b7e0a519e3d403b .",
            "url": "https://random-tree.github.io/blog/jupyter/image/eda/self-education/covid19/fda/2020/06/05/Part1UnderstandingCOVID19.html",
            "relUrl": "/jupyter/image/eda/self-education/covid19/fda/2020/06/05/Part1UnderstandingCOVID19.html",
            "date": " • Jun 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Image processing on 64bit system",
            "content": "Summary . Fast image processing is important to efficiently research and deploy computer vision models. During my research, i found that Jeremy Howard (creator of fastai and highly admired) uses Pillow-SIMD ( a more effiecient version of Pillow library). Furthermore, the awesome Pytorch library uses pillow. So, I set to do a small experiment to compare OpenCV, which is my standard go to library for image processing with Pillow-SIMD. . I have made this comparison on a 64 bit system since they are more capable and tend to be used more often in high computational systems. . Summary results based on average of 7 runs and 100 loops each . loading : OpenCV is 34.0 times slower than PillowSIMD | flipping : OpenCV is 1.32 times faster than PillowSIMD | resizing : OpenCV is 1.34 times faster than PillowSIMD | blurring : OpenCV is 2.42 times faster than PillowSIMD | rotation : OpenCV is 1.74 times slower than PillowSIMD | The OpenCV library in significantly slower while loading images which is good reason for using Pillow-SIMD. Perhaps the reason why Jeremy Howard and Pytorch use Pillow. . Code and Analysis below . import PIL import cv2 import matplotlib.pyplot as plt import numpy as np . Checking the versions of OpenCV and Pillow-SIMD . #collapse-hide print(f&quot;OpenCV version is : {cv2.__version__}&quot;) print(f&quot;Pillow-SIMD version is : {PIL.__version__}&quot;) . . OpenCV version is : 4.1.2 Pillow-SIMD version is : 7.0.0.post3 . the post3 in the version number 7.0.0 implies SIMD installation . Checking system architecture . #collapse-hide print(f&quot;The platform architecture is : {platform.architecture()}&quot;) . . The platform architecture is : (&#39;64bit&#39;, &#39;&#39;) . We need to make sure that the &#39;libjpeg_turbo&#39;is being used instead of &#39;libjpeg&#39; library. libjpeg-turbo replacement for libjpeg is optimized for SIMD. . from PIL import features; print(features.check_feature(&#39;libjpeg_turbo&#39;)) . True . from PIL import Image, ImageOps, ImageFilter . Since, everyone&#39;s favorite Monalisa has been holding the same selfie pose since possibly 1503, i thought of introducing her to a few transformations . Loading Monalisa . Pillow-SIMD . %%timeit -n 100 image_pil = Image.open(image_location) . 627 µs ± 96 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . fig = plt.figure(figsize=(10,10)) plt.imshow(image_pil) plt.show() . OpenCV . %%timeit -n 100 image_cv2 = cv2.imread(image_location) . 21.1 ms ± 472 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . fig = plt.figure(figsize=(10,10)) plt.imshow(image_cv2) plt.show() . Flipping Monalisa . Pillow-SIMD . %%timeit -n 100 flipped_pil_image = ImageOps.flip(image_pil) . 777 µs ± 146 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . fig = plt.figure(figsize=(10,10)) plt.imshow(flipped_pil_image) plt.show() . OpenCV . %%timeit -n 100 flipped_cv2_image = cv2.flip(image_cv2,0) . 590 µs ± 121 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . fig = plt.figure(figsize=(10,10)) plt.imshow(flipped_cv2_image) plt.show() . Resizing Monalisa . Pillow-SIMD . %%timeit -n 100 resized_pil = image_pil.resize((1600,2384)) # original (800, 1192) . 16.1 ms ± 343 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . fig = plt.figure(figsize=(10,10)) plt.imshow(resized_pil) plt.show() . OpenCV . %%timeit -n 100 resized_cv2 = cv2.resize(image_cv2,None,fx=2, fy=2) . 12 ms ± 403 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . fig = plt.figure(figsize=(10,10)) plt.imshow(resized_cv2) plt.show() . Blurring Monalisa . Pillow-SIMD . %%timeit -n 100 blurred_pil = image_pil.filter(ImageFilter.BoxBlur(20)) . 10.5 ms ± 182 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . fig = plt.figure(figsize=(10,10)) plt.imshow(blurred_pil) plt.show() . OpenCV . %%timeit -n 100 blurred_cv2 = cv2.blur(image_cv2,(20,20)) . 4.34 ms ± 142 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . fig = plt.figure(figsize=(10,10)) plt.imshow(blurred_cv2) plt.show() . Rotating Monalisa . Pillow-SIMD . %%timeit -n 100 rotated_pil = image_pil.rotate(-90,expand=True) . 3.4 ms ± 145 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . fig = plt.figure(figsize=(10,10)) plt.imshow(rotated_pil) plt.show() . OpenCV . %%timeit -n 100 rotated_cv2 = cv2.rotate(image_cv2,cv2.ROTATE_90_CLOCKWISE) . 5.93 ms ± 244 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . fig = plt.figure(figsize=(10,10)) plt.imshow(rotated_cv2) plt.show() .",
            "url": "https://random-tree.github.io/blog/jupyter/image%20processing/opencv/pillow-simd/2020/05/15/OpenCVvsPillowSIMD.html",
            "relUrl": "/jupyter/image%20processing/opencv/pillow-simd/2020/05/15/OpenCVvsPillowSIMD.html",
            "date": " • May 15, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I am a passionate machine learning engineer who enjoys generating insights from data, building and deploying machine learning models . Hopefully the work shared via this blog will be useful to the community. .",
          "url": "https://random-tree.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://random-tree.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}